开始准备一个比赛，天池大数据。<br>
先拿一个以往的题练练手，拿了15年的微博互动数预测的题。<br>
初赛的题是根据以往发布的微博预测以后发布的微博的转发数、评论数和点赞数。<br>
训练数据包括：用户ID，微博ID，发布时间，发布一周后的转发数，评论数，点赞数，微博内容。<br>
我们发现：<br>
训练数据中的用户ID和测试数据中的用户ID是相同的，所以找出两个用户特征，一个是用户以往获得的平均互动数，作为以后预测的先验数据。<br>
第二个是用户的发布微博的频率。<br>
微博ID是全局唯一的，没有什么信息可供提取<br>
发布时间的格式是年-月-日 时：分：秒。通过数据可视化，我们观察到年月日的数据是没有用的，小时数据对互动数的影响很大。<br>
所以我们提取了一个特征：把小时数据归一化，即除以24。<br>
微博内容是一个大头，太多重要的信息都在内容里。<br>
首先提取了前50个高频词汇，然后根据每条微博中这50个词汇每一个的出现个数和词汇在整体的频率的乘积，构成50维特征向量<br>
微博中出现的表情的个数，即格式为[cry..]，类似的这样的个数。<br>
微博中URL的个数，即http，https的个数。<br>
有没有标题，即格式为【哈哈哈】,类似的这样。<br>
微博中hash_tag的个数，即#的个数除以2，hash_tag中间的内容即该条微博参与的话题。<br>
微博中@的数量，@人就会有人回应，增加互动数。<br>
微博字符串的长度，要归一化，除以140。<br>
以上，总共提取了59维特征。<br>
<table>
<tr><td>前50个高频词构成的50维向量</td></tr>
<tr><td>表情个数</td></tr>
<tr><td>URL个数</td></tr>
<tr><td>是否有标题</td></tr>
<tr><td>hash_tag个数</td></tr>
<tr><td>@个数</td></tr>
<tr><td>字符串长度，归一化</td></tr>
<tr><td>发布时间的归一化</td></tr>
<tr><td>用户平均互动数</td></tr>
<tr><td>用户发微博的频率</td></tr>
</table>
<br><br>
但是在提取特征之前还要进行数据的清洗整理。<br>
首先把每条微博中的表情个数统计之后，要删掉表情，防止它在提取高频词时有干扰。<br>
要去掉URL后面跟着的@官方微博，因为官微很少有回应。<br>
还要注意停用词表的选取，要尽量的全。<br>
<br><br>
整个处理流程：
逐行读取训练文件，记录每条微博的表情数量，URL数量，删掉url和后面的@官微，删掉表情。<br>
统计每条微博的hash_tag数量，发布时间归一化，是否有标题。<br>
把这些特征加到特征矩阵中。<br>
同时记录每个用户的发微博个数，三种互动数。<br>
处理过的每条微博存到一个大数组里，留待后面使用，减少磁盘IO。<br>
计算每个用户的平均互动数和发微博频率。<br>
统计整个训练数据的前50个高频词。<br>
遍历微博数组，构建50维词汇特征。<br>
统计字符串长度的归一化，统计@数量，加入到特征矩阵。<br>
<br><br>
随机的把weibo_train_data.txt里的数据分成了两个文件，60%放在了train_train_data用来训练。40%放在了train_test_data用来测试模型<br>
清除url时用的正则表达式替换。'http[s]*://(.*)/[a-zA-Z0-9]+'这个正则能匹配任意的短URL。<br>
