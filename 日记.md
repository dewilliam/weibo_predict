开始准备一个比赛，天池大数据。<br>
先拿一个以往的题练练手，拿了15年的微博互动数预测的题。<br>
初赛的题是根据以往发布的微博预测以后发布的微博的转发数、评论数和点赞数。<br>
训练数据包括：用户ID，微博ID，发布时间，发布一周后的转发数，评论数，点赞数，微博内容。<br>
我们发现：<br>
训练数据中的用户ID和测试数据中的用户ID是相同的，所以找出两个用户特征，一个是用户以往获得的平均互动数，作为以后预测的先验数据。<br>
第二个是用户的发布微博的频率。<br>
微博ID是全局唯一的，没有什么信息可供提取<br>
发布时间的格式是年-月-日 时：分：秒。通过数据可视化，我们观察到年月日的数据是没有用的，小时数据对互动数的影响很大。<br>
所以我们提取了一个特征：把小时数据归一化，即除以24。<br>
微博内容是一个大头，太多重要的信息都在内容里。<br>
首先提取了前50个高频词汇，然后根据每条微博中这50个词汇每一个的出现个数和词汇在整体的频率的乘积，构成50维特征向量<br>
微博中出现的表情的个数，即格式为[cry..]，类似的这样的个数。<br>
微博中URL的个数，即http，https的个数。<br>
有没有标题，即格式为【哈哈哈】,类似的这样。<br>
微博中hash_tag的个数，即#的个数除以2，hash_tag中间的内容即该条微博参与的话题。<br>
微博中@的数量，@人就会有人回应，增加互动数。<br>
微博字符串的长度，要归一化，除以140。<br>
以上，总共提取了59维特征。<br>
<table>
<tr><td>前50个高频词构成的50维向量</td></tr>
<tr><td>表情个数</td></tr>
<tr><td>URL个数</td></tr>
<tr><td>是否有标题</td></tr>
<tr><td>hash_tag个数</td></tr>
<tr><td>@个数</td></tr>
<tr><td>字符串长度，归一化</td></tr>
<tr><td>发布时间的归一化</td></tr>
<tr><td>用户平均互动数</td></tr>
<tr><td>用户发微博的频率</td></tr>
</table>
<br><br>
但是在提取特征之前还要进行数据的清洗整理。<br>
首先把每条微博中的表情个数统计之后，要删掉表情，防止它在提取高频词时有干扰。<br>
要去掉URL后面跟着的@官方微博，因为官微很少有回应。<br>
还要注意停用词表的选取，要尽量的全。<br>
<br><br>
整个处理流程：
逐行读取训练文件，记录每条微博的表情数量，URL数量，删掉url和后面的@官微，删掉表情。<br>
统计每条微博的hash_tag数量，发布时间归一化，是否有标题。<br>
把这些特征加到特征矩阵中。<br>
同时记录每个用户的发微博个数，三种互动数。<br>
处理过的每条微博存到一个大数组里，留待后面使用，减少磁盘IO。<br>
计算每个用户的平均互动数和发微博频率。<br>
统计整个训练数据的前50个高频词。<br>
遍历微博数组，构建50维词汇特征。<br>
统计字符串长度的归一化，统计@数量，加入到特征矩阵。<br>
<br><br>
随机的把weibo_train_data.txt里的数据分成了两个文件，60%放在了train_train_data用来训练。40%放在了train_test_data用来测试模型<br>
清除url时用的正则表达式替换。'http[s]*://(.*)/[a-zA-Z0-9]+'这个正则能匹配任意的短URL。<br>
停用词折腾了好久。<br>
要把分词中的噪声词去掉。<br>
噪声词有几种：系统自动生成的微博语句中的主要词语，比如转发东西、抢红包、打车分享等等行为产生的。<br>
&emsp;&emsp;&emsp;&emsp;具有强烈时效性的行为产生的主要词语，比如抢红包，发红包，抽奖，某个公司的节日活动等等<br>
&emsp;&emsp;&emsp;&emsp;具有某一段时间的独特性的活动，比如当时很火的电视剧，电影，演唱会等等的关键词。<br>
最后还要把一些以上格式化清除不能清除掉的噪声清除掉，比如：<br>
line=re.sub('@RAIN-JIHOON','',line)<br>
line=re.sub('@支付宝钱包','',line)<br>
用一个字典来保存平均评论数、转发数、点赞数<br>
KNN的加权平均算法是用公式：Σ(1-this_d/whole_d)*this_target/K<br>
this_d是该向量与测试向量的距离，whole_d是所有K个点的总距离，this_target是测试该向量的目标值。
用该公式来预测测试点的目标值。<br>
评价指标为：[nr-pow(10,ceil(log(nr,10)))/M,nr+pow(10,ceil(log(nr,10)))/M]，在该区间内的视为正确，否则错误。<br>
其中，nr 为正确的目标值，M为控制置信区间长度的参数。<br>
经测试，评论数在K=6时正确率达到最高，为84.5%<br>
而转发数和点赞数在K=2时正确率达到最高，为81.7%和77.9%<br>
用K=2测试，评论数的正确率为84.2%<br>
而方差分别为：13.9%，2.4%，5.9%<br>
反而正确率较低的两个目标值的方差更低。这值得思考一下<br>
这可能意味着,转发数和点赞数的分布更密集，两个点的距离加权平均算法对评论数的影响更大，对转发数和点赞数的影响较小。<br>
如果调整M的话，使置信区间的长度增加6%，则转发数和点赞数正确率都能到100%<br>
<br>
大概8分钟算法能跑完，效率还是可以的<br>
<br>
下一步计划做三个事：<br>
&emsp;&emsp;1.做一下特征选择，特征的效果对KNN的影响还是很大的。<br>
&emsp;&emsp;2.调整一下距离加权平均的计算方法，现在只考虑到了距离因素，没有考虑到被加权的点的可靠程度。<br>
打算加一个靠谱因子，把被K圈起来的点对应的用户的稳定程度加进去，用户稳定，这个点的目标值的采用可靠度会更高。<br>
具体因子怎么加，明天在想。<br>
&emsp;&emsp;3.试一试情感分析，用表情来训练一个模型。在特征里加入情感分类（积极，消极，中性）。
<h2>16年7月1</h2>
距离加权平均的公式要改，单纯看距离因子的和就不为1，这样就放大了目标值。<br>
所以改变成：Σ(exp(-this_d)/Σ(exp(-di)))*this_target
为了加入用户的靠谱因子，整个算法的流程就要有小改动，原来遍历两遍数据就可以，现在要遍历三遍。<br>
第一遍：把需要清除的东西清除掉。统计除词汇特征之外的所有特征。把所有微博内容汇总起来，用户提取50个词汇<br>
然后统计50个高频词。<br>
计算用户的平均互动数，用户的发微博频率。<br>
第二遍：依据高频词，构建每条微博数据的词汇特征向量。统计用户的评论、转发、点赞的标准差。<br>
计算三个指标的标准差。归一化用户发微博频率。<br>
第三遍：把用户发微博频率加入到特征向量中，用三个数组存储三个指标的标准差。<br>
至此，所有特征和距离加权平均的准备数据就计算完毕。<br>
下面就开始测试数据了，流程没有改动。只是把加权公式改动了。<br>
这样改的效果并不好，正确率反而有小的下降。。。。<br>
打算把用户的稳定程度因子加到特征中，在KNN选择近邻点时就考虑这个因素。<br>
<br><br>
改了一下模型，在kNN选出K个近邻点后，用这K个点做一个线性回归。<br>
这样正确率有一丁点的增长，其实线性回归和距离加权平均在本质上是一样的，正确率增长可能是加权平均的公式没写好。<br>
